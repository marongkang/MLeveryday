{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UNet.ipynb","provenance":[],"mount_file_id":"12pvWPzSg24ux0wQmz-10CFgLrXzcCB3L","authorship_tag":"ABX9TyPSt4DcZ9pTO3ZJj86Xl5Zb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#transform"],"metadata":{"id":"Sa9Bn_pUZilC"}},{"cell_type":"code","source":["import math\n","import numbers\n","import random\n","import warnings\n","from collections.abc import Sequence\n","from typing import Tuple, List, Optional\n","\n","import torch\n","from PIL import Image\n","from torch import Tensor\n","\n","try:\n","    import accimage\n","except ImportError:\n","    accimage = None\n","\n","from torchvision.transforms import functional as F\n","\n","__all__ = [\"Compose\", \"ToTensor\", \"PILToTensor\", \"ConvertImageDtype\", \"ToPILImage\", \"Normalize\", \"Resize\", \"Scale\",\n","           \"CenterCrop\", \"Pad\", \"Lambda\", \"RandomApply\", \"RandomChoice\", \"RandomOrder\", \"RandomCrop\",\n","           \"RandomHorizontalFlip\", \"RandomVerticalFlip\", \"RandomResizedCrop\", \"RandomSizedCrop\", \"FiveCrop\", \"TenCrop\",\n","           \"LinearTransformation\", \"ColorJitter\", \"RandomRotation\", \"RandomAffine\", \"Grayscale\", \"RandomGrayscale\",\n","           \"RandomPerspective\", \"RandomErasing\", \"GaussianBlur\"]\n","\n","_pil_interpolation_to_str = {\n","    Image.NEAREST: 'PIL.Image.NEAREST',\n","    Image.BILINEAR: 'PIL.Image.BILINEAR',\n","    Image.BICUBIC: 'PIL.Image.BICUBIC',\n","    Image.LANCZOS: 'PIL.Image.LANCZOS',\n","    Image.HAMMING: 'PIL.Image.HAMMING',\n","    Image.BOX: 'PIL.Image.BOX',\n","}\n","\n","\n","class Compose:\n","\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img1, label):\n","        for t in self.transforms:\n","            img1,  label = t(img1,  label)\n","        return img1,  label\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '('\n","        for t in self.transforms:\n","            format_string += '\\n'\n","            format_string += '    {0}'.format(t)\n","        format_string += '\\n)'\n","        return format_string\n","\n","\n","class ToTensor:\n","\n","    def __call__(self, pic):\n","       \n","        return F.to_tensor(pic)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '()'\n","\n","\n","class PILToTensor:\n","\n","    def __call__(self, pic):\n","        return F.pil_to_tensor(pic)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '()'\n","\n","\n","class ConvertImageDtype(torch.nn.Module):  \n","    def __init__(self, dtype: torch.dtype) -> None:\n","        super().__init__()\n","        self.dtype = dtype\n","\n","    def forward(self, image):\n","        return F.convert_image_dtype(image, self.dtype)\n","\n","\n","class ToPILImage:\n","    def __init__(self, mode=None):\n","        self.mode = mode\n","\n","    def __call__(self, pic):\n","        return F.to_pil_image(pic, self.mode)\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '('\n","        if self.mode is not None:\n","            format_string += 'mode={0}'.format(self.mode)\n","        format_string += ')'\n","        return format_string\n","\n","\n","class Normalize(torch.nn.Module):\n","    def __init__(self, mean, std, inplace=False):\n","        super().__init__()\n","        self.mean = mean\n","        self.std = std\n","        self.inplace = inplace\n","\n","    def forward(self, tensor: Tensor) -> Tensor:\n","        return F.normalize(tensor, self.mean, self.std, self.inplace)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n","\n","\n","class Resize(torch.nn.Module):\n","\n","    def __init__(self, size, interpolation=Image.BILINEAR):\n","        super().__init__()\n","        if not isinstance(size, (int, Sequence)):\n","            raise TypeError(\"Size should be int or sequence. Got {}\".format(type(size)))\n","        if isinstance(size, Sequence) and len(size) not in (1, 2):\n","            raise ValueError(\"If size is a sequence, it should have 1 or 2 values\")\n","        self.size = size\n","        self.interpolation = interpolation\n","\n","    def forward(self, img):\n","        return F.resize(img, self.size, self.interpolation)\n","\n","    def __repr__(self):\n","        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n","        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n","\n","\n","class Scale(Resize):\n","    def __init__(self, *args, **kwargs):\n","        warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n","                      \"please use transforms.Resize instead.\")\n","        super(Scale, self).__init__(*args, **kwargs)\n","\n","\n","class CenterCrop(torch.nn.Module):\n","\n","    def __init__(self, size):\n","        super().__init__()\n","        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n","\n","    def forward(self, img):\n","        return F.center_crop(img, self.size)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(size={0})'.format(self.size)\n","\n","\n","class Pad(torch.nn.Module):\n","\n","    def __init__(self, padding, fill=0, padding_mode=\"constant\"):\n","        super().__init__()\n","        if not isinstance(padding, (numbers.Number, tuple, list)):\n","            raise TypeError(\"Got inappropriate padding arg\")\n","\n","        if not isinstance(fill, (numbers.Number, str, tuple)):\n","            raise TypeError(\"Got inappropriate fill arg\")\n","\n","        if padding_mode not in [\"constant\", \"edge\", \"reflect\", \"symmetric\"]:\n","            raise ValueError(\"Padding mode should be either constant, edge, reflect or symmetric\")\n","\n","        if isinstance(padding, Sequence) and len(padding) not in [1, 2, 4]:\n","            raise ValueError(\"Padding must be an int or a 1, 2, or 4 element tuple, not a \" +\n","                             \"{} element tuple\".format(len(padding)))\n","\n","        self.padding = padding\n","        self.fill = fill\n","        self.padding_mode = padding_mode\n","\n","    def forward(self, img):\n","        return F.pad(img, self.padding, self.fill, self.padding_mode)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n","            format(self.padding, self.fill, self.padding_mode)\n","\n","\n","class Lambda:\n","    def __init__(self, lambd):\n","        if not callable(lambd):\n","            raise TypeError(\"Argument lambd should be callable, got {}\".format(repr(type(lambd).__name__)))\n","        self.lambd = lambd\n","\n","    def __call__(self, img):\n","        return self.lambd(img)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '()'\n","\n","\n","class RandomTransforms:\n","\n","    def __init__(self, transforms):\n","        if not isinstance(transforms, Sequence):\n","            raise TypeError(\"Argument transforms should be a sequence\")\n","        self.transforms = transforms\n","\n","    def __call__(self, *args, **kwargs):\n","        raise NotImplementedError()\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '('\n","        for t in self.transforms:\n","            format_string += '\\n'\n","            format_string += '    {0}'.format(t)\n","        format_string += '\\n)'\n","        return format_string\n","\n","\n","class RandomApply(torch.nn.Module):\n","\n","    def __init__(self, transforms, p=0.5):\n","        super().__init__()\n","        self.transforms = transforms\n","        self.p = p\n","\n","    def forward(self, img):\n","        if self.p < torch.rand(1):\n","            return img\n","        for t in self.transforms:\n","            img = t(img)\n","        return img\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '('\n","        format_string += '\\n    p={}'.format(self.p)\n","        for t in self.transforms:\n","            format_string += '\\n'\n","            format_string += '    {0}'.format(t)\n","        format_string += '\\n)'\n","        return format_string\n","\n","\n","class RandomOrder(RandomTransforms):\n","    def __call__(self, img):\n","        order = list(range(len(self.transforms)))\n","        random.shuffle(order)\n","        for i in order:\n","            img = self.transforms[i](img)\n","        return img\n","\n","\n","class RandomChoice(RandomTransforms):\n","\n","    def __call__(self, img):\n","        t = random.choice(self.transforms)\n","        return t(img)\n","\n","\n","class RandomCrop(torch.nn.Module):\n","    @staticmethod\n","    def get_params(img: Tensor, output_size: Tuple[int, int]) -> Tuple[int, int, int, int]:\n","        w, h = F._get_image_size(img)\n","        th, tw = output_size\n","\n","        if h + 1 < th or w + 1 < tw:\n","            raise ValueError(\n","                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n","            )\n","\n","        if w == tw and h == th:\n","            return 0, 0, h, w\n","\n","        i = torch.randint(0, h - th + 1, size=(1, )).item()\n","        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n","        return i, j, th, tw\n","\n","    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode=\"constant\"):\n","        super().__init__()\n","\n","        self.size = tuple(_setup_size(\n","            size, error_msg=\"Please provide only two dimensions (h, w) for size.\"\n","        ))\n","\n","        self.padding = padding\n","        self.pad_if_needed = pad_if_needed\n","        self.fill = fill\n","        self.padding_mode = padding_mode\n","\n","    def forward(self, img):\n","        if self.padding is not None:\n","            img = F.pad(img, self.padding, self.fill, self.padding_mode)\n","\n","        width, height = F._get_image_size(img)\n","        # pad the width if needed\n","        if self.pad_if_needed and width < self.size[1]:\n","            padding = [self.size[1] - width, 0]\n","            img = F.pad(img, padding, self.fill, self.padding_mode)\n","        # pad the height if needed\n","        if self.pad_if_needed and height < self.size[0]:\n","            padding = [0, self.size[0] - height]\n","            img = F.pad(img, padding, self.fill, self.padding_mode)\n","\n","        i, j, h, w = self.get_params(img, self.size)\n","\n","        return F.crop(img, i, j, h, w)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + \"(size={0}, padding={1})\".format(self.size, self.padding)\n","\n","\n","class RandomHorizontalFlip(torch.nn.Module):\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","        self.p = p\n","\n","    def forward(self, img1,  label):\n","        if torch.rand(1) < self.p:\n","            return F.hflip(img1),  F.hflip(label)\n","        return img1, label\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(p={})'.format(self.p)\n","\n","\n","class RandomVerticalFlip(torch.nn.Module):\n","\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","        self.p = p\n","\n","    def forward(self, img1, label):\n","        if torch.rand(1) < self.p:\n","            return F.vflip(img1),  F.vflip(label)\n","        return img1,  label\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(p={})'.format(self.p)\n","\n","\n","class RandomPerspective(torch.nn.Module):\n","    def __init__(self, distortion_scale=0.5, p=0.5, interpolation=Image.BILINEAR, fill=0):\n","        super().__init__()\n","        self.p = p\n","        self.interpolation = interpolation\n","        self.distortion_scale = distortion_scale\n","        self.fill = fill\n","\n","    def forward(self, img):\n","        if torch.rand(1) < self.p:\n","            width, height = F._get_image_size(img)\n","            startpoints, endpoints = self.get_params(width, height, self.distortion_scale)\n","            return F.perspective(img, startpoints, endpoints, self.interpolation, self.fill)\n","        return img\n","\n","    @staticmethod\n","    def get_params(width: int, height: int, distortion_scale: float) -> Tuple[List[List[int]], List[List[int]]]:\n","        \"\"\"Get parameters for ``perspective`` for a random perspective transform.\n","\n","        Args:\n","            width (int): width of the image.\n","            height (int): height of the image.\n","            distortion_scale (float): argument to control the degree of distortion and ranges from 0 to 1.\n","\n","        Returns:\n","            List containing [top-left, top-right, bottom-right, bottom-left] of the original image,\n","            List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image.\n","        \"\"\"\n","        half_height = height // 2\n","        half_width = width // 2\n","        topleft = [\n","            int(torch.randint(0, int(distortion_scale * half_width) + 1, size=(1, )).item()),\n","            int(torch.randint(0, int(distortion_scale * half_height) + 1, size=(1, )).item())\n","        ]\n","        topright = [\n","            int(torch.randint(width - int(distortion_scale * half_width) - 1, width, size=(1, )).item()),\n","            int(torch.randint(0, int(distortion_scale * half_height) + 1, size=(1, )).item())\n","        ]\n","        botright = [\n","            int(torch.randint(width - int(distortion_scale * half_width) - 1, width, size=(1, )).item()),\n","            int(torch.randint(height - int(distortion_scale * half_height) - 1, height, size=(1, )).item())\n","        ]\n","        botleft = [\n","            int(torch.randint(0, int(distortion_scale * half_width) + 1, size=(1, )).item()),\n","            int(torch.randint(height - int(distortion_scale * half_height) - 1, height, size=(1, )).item())\n","        ]\n","        startpoints = [[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]]\n","        endpoints = [topleft, topright, botright, botleft]\n","        return startpoints, endpoints\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(p={})'.format(self.p)\n","\n","\n","class RandomResizedCrop(torch.nn.Module):\n","    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR):\n","        super().__init__()\n","        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n","\n","        if not isinstance(scale, Sequence):\n","            raise TypeError(\"Scale should be a sequence\")\n","        if not isinstance(ratio, Sequence):\n","            raise TypeError(\"Ratio should be a sequence\")\n","        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n","            warnings.warn(\"Scale and ratio should be of kind (min, max)\")\n","\n","        self.interpolation = interpolation\n","        self.scale = scale\n","        self.ratio = ratio\n","\n","    @staticmethod\n","    def get_params(\n","            img: Tensor, scale: List[float], ratio: List[float]\n","    ) -> Tuple[int, int, int, int]:\n","        # width, height = F._get_image_size(img)\n","        width, height = img.size\n","        area = height * width\n","\n","        for _ in range(10):\n","            target_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n","            log_ratio = torch.log(torch.tensor(ratio))\n","            aspect_ratio = torch.exp(\n","                torch.empty(1).uniform_(log_ratio[0], log_ratio[1])\n","            ).item()\n","\n","            w = int(round(math.sqrt(target_area * aspect_ratio)))\n","            h = int(round(math.sqrt(target_area / aspect_ratio)))\n","\n","            if 0 < w <= width and 0 < h <= height:\n","                i = torch.randint(0, height - h + 1, size=(1,)).item()\n","                j = torch.randint(0, width - w + 1, size=(1,)).item()\n","                return i, j, h, w\n","\n","        # Fallback to central crop\n","        in_ratio = float(width) / float(height)\n","        if in_ratio < min(ratio):\n","            w = width\n","            h = int(round(w / min(ratio)))\n","        elif in_ratio > max(ratio):\n","            h = height\n","            w = int(round(h * max(ratio)))\n","        else:  # whole image\n","            w = width\n","            h = height\n","        i = (height - h) // 2\n","        j = (width - w) // 2\n","        return i, j, h, w\n","\n","    def forward(self, img1,  label):\n","        i, j, h, w = self.get_params(img1, self.scale, self.ratio)\n","        return F.resized_crop(img1, i, j, h, w, self.size, self.interpolation), F.resized_crop(label, i, j, h, w, self.size, self.interpolation),\n","\n","    def __repr__(self):\n","        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n","        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n","        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n","        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n","        format_string += ', interpolation={0})'.format(interpolate_str)\n","        return format_string\n","\n","\n","class RandomSizedCrop(RandomResizedCrop):\n","    def __init__(self, *args, **kwargs):\n","        warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n","                      \"please use transforms.RandomResizedCrop instead.\")\n","        super(RandomSizedCrop, self).__init__(*args, **kwargs)\n","\n","\n","class FiveCrop(torch.nn.Module):\n","    def __init__(self, size):\n","        super().__init__()\n","        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n","\n","    def forward(self, img):\n","        return F.five_crop(img, self.size)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(size={0})'.format(self.size)\n","\n","\n","class TenCrop(torch.nn.Module):\n","\n","    def __init__(self, size, vertical_flip=False):\n","        super().__init__()\n","        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n","        self.vertical_flip = vertical_flip\n","\n","    def forward(self, img):\n","        return F.ten_crop(img, self.size, self.vertical_flip)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(size={0}, vertical_flip={1})'.format(self.size, self.vertical_flip)\n","\n","\n","class LinearTransformation(torch.nn.Module):\n","\n","    def __init__(self, transformation_matrix, mean_vector):\n","        super().__init__()\n","        if transformation_matrix.size(0) != transformation_matrix.size(1):\n","            raise ValueError(\"transformation_matrix should be square. Got \" +\n","                             \"[{} x {}] rectangular matrix.\".format(*transformation_matrix.size()))\n","\n","        if mean_vector.size(0) != transformation_matrix.size(0):\n","            raise ValueError(\"mean_vector should have the same length {}\".format(mean_vector.size(0)) +\n","                             \" as any one of the dimensions of the transformation_matrix [{}]\"\n","                             .format(tuple(transformation_matrix.size())))\n","\n","        if transformation_matrix.device != mean_vector.device:\n","            raise ValueError(\"Input tensors should be on the same device. Got {} and {}\"\n","                             .format(transformation_matrix.device, mean_vector.device))\n","\n","        self.transformation_matrix = transformation_matrix\n","        self.mean_vector = mean_vector\n","\n","    def forward(self, tensor: Tensor) -> Tensor:\n","        shape = tensor.shape\n","        n = shape[-3] * shape[-2] * shape[-1]\n","        if n != self.transformation_matrix.shape[0]:\n","            raise ValueError(\"Input tensor and transformation matrix have incompatible shape.\" +\n","                             \"[{} x {} x {}] != \".format(shape[-3], shape[-2], shape[-1]) +\n","                             \"{}\".format(self.transformation_matrix.shape[0]))\n","\n","        if tensor.device.type != self.mean_vector.device.type:\n","            raise ValueError(\"Input tensor should be on the same device as transformation matrix and mean vector. \"\n","                             \"Got {} vs {}\".format(tensor.device, self.mean_vector.device))\n","\n","        flat_tensor = tensor.view(-1, n) - self.mean_vector\n","        transformed_tensor = torch.mm(flat_tensor, self.transformation_matrix)\n","        tensor = transformed_tensor.view(shape)\n","        return tensor\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '(transformation_matrix='\n","        format_string += (str(self.transformation_matrix.tolist()) + ')')\n","        format_string += (\", (mean_vector=\" + str(self.mean_vector.tolist()) + ')')\n","        return format_string\n","\n","\n","class ColorJitter(torch.nn.Module):\n","    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n","        super().__init__()\n","        self.brightness = self._check_input(brightness, 'brightness')\n","        self.contrast = self._check_input(contrast, 'contrast')\n","        self.saturation = self._check_input(saturation, 'saturation')\n","        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n","                                     clip_first_on_zero=False)\n","\n","    @torch.jit.unused\n","    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n","        if isinstance(value, numbers.Number):\n","            if value < 0:\n","                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n","            value = [center - float(value), center + float(value)]\n","            if clip_first_on_zero:\n","                value[0] = max(value[0], 0.0)\n","        elif isinstance(value, (tuple, list)) and len(value) == 2:\n","            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n","                raise ValueError(\"{} values should be between {}\".format(name, bound))\n","        else:\n","            raise TypeError(\"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n","\n","        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n","        # or (0., 0.) for hue, do nothing\n","        if value[0] == value[1] == center:\n","            value = None\n","        return value\n","\n","    @staticmethod\n","    @torch.jit.unused\n","    def get_params(brightness, contrast, saturation, hue):\n","        \"\"\"Get a randomized transform to be applied on image.\n","\n","        Arguments are same as that of __init__.\n","\n","        Returns:\n","            Transform which randomly adjusts brightness, contrast and\n","            saturation in a random order.\n","        \"\"\"\n","        transforms = []\n","\n","        if brightness is not None:\n","            brightness_factor = random.uniform(brightness[0], brightness[1])\n","            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n","\n","        if contrast is not None:\n","            contrast_factor = random.uniform(contrast[0], contrast[1])\n","            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n","\n","        if saturation is not None:\n","            saturation_factor = random.uniform(saturation[0], saturation[1])\n","            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n","\n","        if hue is not None:\n","            hue_factor = random.uniform(hue[0], hue[1])\n","            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n","\n","        random.shuffle(transforms)\n","        transform = Compose(transforms)\n","\n","        return transform\n","\n","    def forward(self, img1, label):\n","        fn_idx = torch.randperm(4)\n","        for fn_id in fn_idx:\n","            if fn_id == 0 and self.brightness is not None:\n","                brightness = self.brightness\n","                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n","                img1 = F.adjust_brightness(img1, brightness_factor)\n","\n","\n","            if fn_id == 1 and self.contrast is not None:\n","                contrast = self.contrast\n","                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n","                img1 = F.adjust_contrast(img1, contrast_factor)\n","\n","\n","            if fn_id == 2 and self.saturation is not None:\n","                saturation = self.saturation\n","                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n","                img1 = F.adjust_saturation(img1, saturation_factor)\n","\n","\n","            if fn_id == 3 and self.hue is not None:\n","                hue = self.hue\n","                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n","                img1 = F.adjust_hue(img1, hue_factor)\n","\n","\n","        return img1,  label\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '('\n","        format_string += 'brightness={0}'.format(self.brightness)\n","        format_string += ', contrast={0}'.format(self.contrast)\n","        format_string += ', saturation={0}'.format(self.saturation)\n","        format_string += ', hue={0})'.format(self.hue)\n","        return format_string\n","\n","\n","class RandomRotation(torch.nn.Module):\n","    def __init__(self, degrees, resample=False, expand=False, center=None, fill=None):\n","        super().__init__()\n","        self.degrees = _setup_angle(degrees, name=\"degrees\", req_sizes=(2, ))\n","\n","        if center is not None:\n","            _check_sequence_input(center, \"center\", req_sizes=(2, ))\n","\n","        self.center = center\n","\n","        self.resample = resample\n","        self.expand = expand\n","        self.fill = fill\n","\n","    @staticmethod\n","    def get_params(degrees: List[float]) -> float:\n","        angle = float(torch.empty(1).uniform_(float(degrees[0]), float(degrees[1])).item())\n","        return angle\n","\n","    def forward(self, img1, label):\n","        angle = self.get_params(self.degrees)\n","        return F.rotate(img1, angle, self.resample, self.expand, self.center, self.fill),  F.rotate(label, angle, self.resample, self.expand, self.center, self.fill)\n","\n","\n","\n","    def __repr__(self):\n","        format_string = self.__class__.__name__ + '(degrees={0}'.format(self.degrees)\n","        format_string += ', resample={0}'.format(self.resample)\n","        format_string += ', expand={0}'.format(self.expand)\n","        if self.center is not None:\n","            format_string += ', center={0}'.format(self.center)\n","        if self.fill is not None:\n","            format_string += ', fill={0}'.format(self.fill)\n","        format_string += ')'\n","        return format_string\n","\n","\n","class RandomAffine(torch.nn.Module):\n","    def __init__(self, degrees, translate=None, scale=None, shear=None, resample=0, fillcolor=0):\n","        super().__init__()\n","        self.degrees = _setup_angle(degrees, name=\"degrees\", req_sizes=(2, ))\n","\n","        if translate is not None:\n","            _check_sequence_input(translate, \"translate\", req_sizes=(2, ))\n","            for t in translate:\n","                if not (0.0 <= t <= 1.0):\n","                    raise ValueError(\"translation values should be between 0 and 1\")\n","        self.translate = translate\n","\n","        if scale is not None:\n","            _check_sequence_input(scale, \"scale\", req_sizes=(2, ))\n","            for s in scale:\n","                if s <= 0:\n","                    raise ValueError(\"scale values should be positive\")\n","        self.scale = scale\n","\n","        if shear is not None:\n","            self.shear = _setup_angle(shear, name=\"shear\", req_sizes=(2, 4))\n","        else:\n","            self.shear = shear\n","\n","        self.resample = resample\n","        self.fillcolor = fillcolor\n","\n","    @staticmethod\n","    def get_params(\n","            degrees: List[float],\n","            translate: Optional[List[float]],\n","            scale_ranges: Optional[List[float]],\n","            shears: Optional[List[float]],\n","            img_size: List[int]\n","    ) -> Tuple[float, Tuple[int, int], float, Tuple[float, float]]:\n","        angle = float(torch.empty(1).uniform_(float(degrees[0]), float(degrees[1])).item())\n","        if translate is not None:\n","            max_dx = float(translate[0] * img_size[0])\n","            max_dy = float(translate[1] * img_size[1])\n","            tx = int(round(torch.empty(1).uniform_(-max_dx, max_dx).item()))\n","            ty = int(round(torch.empty(1).uniform_(-max_dy, max_dy).item()))\n","            translations = (tx, ty)\n","        else:\n","            translations = (0, 0)\n","\n","        if scale_ranges is not None:\n","            scale = float(torch.empty(1).uniform_(scale_ranges[0], scale_ranges[1]).item())\n","        else:\n","            scale = 1.0\n","\n","        shear_x = shear_y = 0.0\n","        if shears is not None:\n","            shear_x = float(torch.empty(1).uniform_(shears[0], shears[1]).item())\n","            if len(shears) == 4:\n","                shear_y = float(torch.empty(1).uniform_(shears[2], shears[3]).item())\n","\n","        shear = (shear_x, shear_y)\n","\n","        return angle, translations, scale, shear\n","\n","    def forward(self, img):\n","        img_size = F._get_image_size(img)\n","\n","        ret = self.get_params(self.degrees, self.translate, self.scale, self.shear, img_size)\n","        return F.affine(img, *ret, resample=self.resample, fillcolor=self.fillcolor)\n","\n","    def __repr__(self):\n","        s = '{name}(degrees={degrees}'\n","        if self.translate is not None:\n","            s += ', translate={translate}'\n","        if self.scale is not None:\n","            s += ', scale={scale}'\n","        if self.shear is not None:\n","            s += ', shear={shear}'\n","        if self.resample > 0:\n","            s += ', resample={resample}'\n","        if self.fillcolor != 0:\n","            s += ', fillcolor={fillcolor}'\n","        s += ')'\n","        d = dict(self.__dict__)\n","        d['resample'] = _pil_interpolation_to_str[d['resample']]\n","        return s.format(name=self.__class__.__name__, **d)\n","\n","\n","class Grayscale(torch.nn.Module):\n","\n","    def __init__(self, num_output_channels=1):\n","        super().__init__()\n","        self.num_output_channels = num_output_channels\n","\n","    def forward(self, img):\n","        return F.rgb_to_grayscale(img, num_output_channels=self.num_output_channels)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(num_output_channels={0})'.format(self.num_output_channels)\n","\n","\n","class RandomGrayscale(torch.nn.Module):\n","    def __init__(self, p=0.1):\n","        super().__init__()\n","        self.p = p\n","\n","    def forward(self, img):\n","        num_output_channels = F._get_image_num_channels(img)\n","        if torch.rand(1) < self.p:\n","            return F.rgb_to_grayscale(img, num_output_channels=num_output_channels)\n","        return img\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(p={0})'.format(self.p)\n","\n","\n","class RandomErasing(torch.nn.Module):\n","    def __init__(self, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False):\n","        super().__init__()\n","        if not isinstance(value, (numbers.Number, str, tuple, list)):\n","            raise TypeError(\"Argument value should be either a number or str or a sequence\")\n","        if isinstance(value, str) and value != \"random\":\n","            raise ValueError(\"If value is str, it should be 'random'\")\n","        if not isinstance(scale, (tuple, list)):\n","            raise TypeError(\"Scale should be a sequence\")\n","        if not isinstance(ratio, (tuple, list)):\n","            raise TypeError(\"Ratio should be a sequence\")\n","        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n","            warnings.warn(\"Scale and ratio should be of kind (min, max)\")\n","        if scale[0] < 0 or scale[1] > 1:\n","            raise ValueError(\"Scale should be between 0 and 1\")\n","        if p < 0 or p > 1:\n","            raise ValueError(\"Random erasing probability should be between 0 and 1\")\n","\n","        self.p = p\n","        self.scale = scale\n","        self.ratio = ratio\n","        self.value = value\n","        self.inplace = inplace\n","\n","    @staticmethod\n","    def get_params(\n","            img: Tensor, scale: Tuple[float, float], ratio: Tuple[float, float], value: Optional[List[float]] = None\n","    ) -> Tuple[int, int, int, int, Tensor]:\n","        img_c, img_h, img_w = img.shape[-3], img.shape[-2], img.shape[-1]\n","        area = img_h * img_w\n","\n","        for _ in range(10):\n","            erase_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()\n","            aspect_ratio = torch.empty(1).uniform_(ratio[0], ratio[1]).item()\n","\n","            h = int(round(math.sqrt(erase_area * aspect_ratio)))\n","            w = int(round(math.sqrt(erase_area / aspect_ratio)))\n","            if not (h < img_h and w < img_w):\n","                continue\n","\n","            if value is None:\n","                v = torch.empty([img_c, h, w], dtype=torch.float32).normal_()\n","            else:\n","                v = torch.tensor(value)[:, None, None]\n","\n","            i = torch.randint(0, img_h - h + 1, size=(1, )).item()\n","            j = torch.randint(0, img_w - w + 1, size=(1, )).item()\n","            return i, j, h, w, v\n","\n","        # Return original image\n","        return 0, 0, img_h, img_w, img\n","\n","    def forward(self, img):\n","        if torch.rand(1) < self.p:\n","\n","            # cast self.value to script acceptable type\n","            if isinstance(self.value, (int, float)):\n","                value = [self.value, ]\n","            elif isinstance(self.value, str):\n","                value = None\n","            elif isinstance(self.value, tuple):\n","                value = list(self.value)\n","            else:\n","                value = self.value\n","\n","            if value is not None and not (len(value) in (1, img.shape[-3])):\n","                raise ValueError(\n","                    \"If value is a sequence, it should have either a single value or \"\n","                    \"{} (number of input channels)\".format(img.shape[-3])\n","                )\n","\n","            x, y, h, w, v = self.get_params(img, scale=self.scale, ratio=self.ratio, value=value)\n","            return F.erase(img, x, y, h, w, v, self.inplace)\n","        return img\n","\n","\n","class GaussianBlur(torch.nn.Module):\n","\n","    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n","        super().__init__()\n","        self.kernel_size = _setup_size(kernel_size, \"Kernel size should be a tuple/list of two integers\")\n","        for ks in self.kernel_size:\n","            if ks <= 0 or ks % 2 == 0:\n","                raise ValueError(\"Kernel size value should be an odd and positive number.\")\n","\n","        if isinstance(sigma, numbers.Number):\n","            if sigma <= 0:\n","                raise ValueError(\"If sigma is a single number, it must be positive.\")\n","            sigma = (sigma, sigma)\n","        elif isinstance(sigma, Sequence) and len(sigma) == 2:\n","            if not 0. < sigma[0] <= sigma[1]:\n","                raise ValueError(\"sigma values should be positive and of the form (min, max).\")\n","        else:\n","            raise ValueError(\"sigma should be a single number or a list/tuple with length 2.\")\n","\n","        self.sigma = sigma\n","\n","    @staticmethod\n","    def get_params(sigma_min: float, sigma_max: float) -> float:\n","        return torch.empty(1).uniform_(sigma_min, sigma_max).item()\n","\n","    def forward(self, img: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            img (PIL Image or Tensor): image to be blurred.\n","\n","        Returns:\n","            PIL Image or Tensor: Gaussian blurred image\n","        \"\"\"\n","        sigma = self.get_params(self.sigma[0], self.sigma[1])\n","        return F.gaussian_blur(img, self.kernel_size, [sigma, sigma])\n","\n","    def __repr__(self):\n","        s = '(kernel_size={}, '.format(self.kernel_size)\n","        s += 'sigma={})'.format(self.sigma)\n","        return self.__class__.__name__ + s\n","\n","\n","def _setup_size(size, error_msg):\n","    if isinstance(size, numbers.Number):\n","        return int(size), int(size)\n","\n","    if isinstance(size, Sequence) and len(size) == 1:\n","        return size[0], size[0]\n","\n","    if len(size) != 2:\n","        raise ValueError(error_msg)\n","\n","    return size\n","\n","\n","def _check_sequence_input(x, name, req_sizes):\n","    msg = req_sizes[0] if len(req_sizes) < 2 else \" or \".join([str(s) for s in req_sizes])\n","    if not isinstance(x, Sequence):\n","        raise TypeError(\"{} should be a sequence of length {}.\".format(name, msg))\n","    if len(x) not in req_sizes:\n","        raise ValueError(\"{} should be sequence of length {}.\".format(name, msg))\n","\n","\n","def _setup_angle(x, name, req_sizes=(2, )):\n","    if isinstance(x, numbers.Number):\n","        if x < 0:\n","            raise ValueError(\"If {} is a single number, it must be positive.\".format(name))\n","        x = [-x, x]\n","    else:\n","        _check_sequence_input(x, name, req_sizes)\n","\n","    return [float(d) for d in x]\n"],"metadata":{"id":"iE7rCqXgZRl9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"EYO4nZoJaUix"}},{"cell_type":"markdown","source":["#utils"],"metadata":{"id":"Ho16dJtsbdeV"}},{"cell_type":"code","source":["import torch\n","import logging\n","import torch.nn as nn\n","import numpy as np\n","from skimage import measure\n","from torch._utils import _accumulate\n","from torch import randperm\n","from scipy.ndimage import morphology\n","\n","\n","def random_split(dataset, lengths, inds=None, israndom=True):\n","    r\"\"\"\n","    Randomly split a dataset into non-overlapping new datasets of given lengths.\n","\n","    Arguments:\n","        dataset (Dataset): Dataset to be split\n","        lengths (sequence): lengths of splits to be produced\n","    \"\"\"\n","    if sum(lengths) != len(dataset):\n","        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n","\n","    if israndom:\n","        indices = randperm(sum(lengths)).tolist()\n","        print(indices)\n","    else:\n","        indices = inds\n","\n","    return [torch.utils.data.Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n","\n","\n","def expand_as_one_hot(input, C, ignore_index=None):\n","    \"\"\"\n","    Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\n","    It is assumed that the batch dimension is present.\n","    Args:\n","        input (torch.Tensor): 3D/4D input image\n","        C (int): number of channels/labels\n","        ignore_index (int): ignore index to be kept during the expansion\n","    Returns:\n","        4D/5D output torch.Tensor (NxCxSPATIAL)\n","    \"\"\"\n","    assert input.dim() == 4\n","\n","    # expand the input tensor to Nx1xSPATIAL before scattering\n","    input = input.unsqueeze(1)\n","    # create output tensor shape (NxCxSPATIAL)\n","    shape = list(input.size())\n","    shape[1] = C\n","\n","    if ignore_index is not None:\n","        # create ignore_index mask for the result\n","        mask = input.expand(shape) == ignore_index\n","        # clone the src tensor and zero out ignore_index in the input\n","        input = input.clone()\n","        input[input == ignore_index] = 0\n","        # scatter to get the one-hot tensor\n","        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n","        # bring back the ignore_index in the result\n","        result[mask] = ignore_index\n","        return result\n","    else:\n","        # scatter to get the one-hot tensor\n","        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n","\n","\n","def random_split(dataset, lengths, inds=None, israndom=True):\n","    r\"\"\"\n","    Randomly split a data into non-overlapping new datasets of given lengths.\n","\n","    Arguments:\n","        dataset (Dataset): Dataset to be split\n","        lengths (sequence): lengths of splits to be produced\n","    \"\"\"\n","    if sum(lengths) != len(dataset):\n","        raise ValueError(\"Sum of input lengths does not equal the length of the input data!\")\n","\n","    if israndom:\n","        indices = randperm(sum(lengths)).tolist()\n","        print(indices)\n","    else:\n","        indices = inds\n","\n","    return [torch.utils.data.Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","\n","def logger(filename, verbosity=1, name=None):\n","    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n","    formatter = logging.Formatter(\n","        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n","    )\n","    logger = logging.getLogger(name)\n","    logger.setLevel(level_dict[verbosity])\n","\n","    fh = logging.FileHandler(filename, \"w\")\n","    fh.setFormatter(formatter)\n","    logger.addHandler(fh)\n","\n","    sh = logging.StreamHandler()\n","    sh.setFormatter(formatter)\n","    logger.addHandler(sh)\n","    return logger\n","\n","\n","def iou_score(output, target):\n","    smooth = 1e-5\n","    if torch.is_tensor(output):\n","        output = torch.sigmoid(output).data.cpu().round().numpy()\n","    if torch.is_tensor(target):\n","        target = target.data.cpu().numpy()\n","    output_ = output > 0.5\n","    target_ = target > 0.5\n","    intersection = (output_ & target_).sum()\n","    union = (output_ | target_).sum()\n","\n","    return (intersection + smooth) / (union + smooth)\n","\n","\n","def dice_coeff(output, target):\n","    smooth = 1e-5\n","\n","    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n","    target = target.view(-1).data.cpu().numpy()\n","    intersection = (output * target).sum()\n","\n","    return (2. * intersection + smooth) / \\\n","        (output.sum() + target.sum() + smooth)\n","\n","\n","class Evaluator(object):\n","    def __init__(self, num_class):\n","        self.num_class = num_class\n","        self.confusion_matrix = np.zeros((self.num_class,)*2)\n","\n","    def Precision(self):\n","        precision = np.diag(self.confusion_matrix)[0]/self.confusion_matrix[:, 0].sum()\n","        return precision\n","\n","    def Recall(self):\n","        recall = np.diag(self.confusion_matrix)[0]/self.confusion_matrix[0, :].sum()\n","        return recall\n","\n","    def Specificity(self):\n","        specificity = np.diag(self.confusion_matrix)[1]/(self.confusion_matrix[0, 1]+self.confusion_matrix[1, 0])\n","        return specificity\n","\n","    def F1score(self):\n","        prec = self.Precision()\n","        rec = self.Recall()\n","        f1_score = (2*prec*rec)/(prec+rec)\n","        return f1_score\n","\n","    def F2score(self):\n","        prec = self.Precision()\n","        rec = self.Recall()\n","        f2_score = (5*prec*rec)/(4*prec+rec)\n","        return f2_score\n","\n","    def Intersection_over_Union(self):\n","        iou = np.diag(self.confusion_matrix)[0]/(self.confusion_matrix[0,0]+self.confusion_matrix[1,0]+self.confusion_matrix[0,1])\n","        return iou\n","\n","    def Pixel_Accuracy(self):\n","        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n","        return Acc\n","\n","    def Pixel_Accuracy_Class(self):\n","        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n","        Acc = np.nanmean(Acc)\n","        return Acc\n","\n","    def Mean_Intersection_over_Union(self):\n","        MIoU = np.diag(self.confusion_matrix) / (\n","                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n","                    np.diag(self.confusion_matrix))\n","        MIoU = np.nanmean(MIoU)\n","        return MIoU\n","\n","    def Frequency_Weighted_Intersection_over_Union(self):\n","        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n","        iu = np.diag(self.confusion_matrix) / (\n","                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n","                    np.diag(self.confusion_matrix))\n","\n","        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n","        return FWIoU\n","\n","    def _generate_matrix(self, gt_image, pre_image):\n","        mask = (gt_image >= 0) & (gt_image < self.num_class)\n","        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n","        count = np.bincount(label, minlength=self.num_class**2)\n","        confusion_matrix = count.reshape(self.num_class, self.num_class)\n","        print(confusion_matrix)\n","        return confusion_matrix\n","\n","    def add_batch(self, gt_image, pre_image):\n","        assert gt_image.shape == pre_image.shape\n","        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n","\n","    def reset(self):\n","        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n","\n","\n","def adjust_lr(optimizer, init_lr, epoch, decay_rate=0.1, decay_epoch=30):\n","    decay = decay_rate ** (epoch // decay_epoch)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] *= decay\n","\n","\n","def clip_gradient(optimizer, grad_clip):\n","    \"\"\"\n","    For calibrating misalignment gradient via cliping gradient technique\n","    :param optimizer:\n","    :param grad_clip:\n","    :return:\n","    \"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group['params']:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)\n","\n","\n","def universal3Dlargestregion(deal):\n","    \"\"\"找到3D丈量最大连通域,输出为值为1的mask.deal:输入的3D张量\"\"\"\n","    labels = measure.label(deal, connectivity=3)  # 找白色区域的8连通域，并给予每个连通域标号，connectivity为ndarry的维数，三维数组故为3\n","    jj = measure.regionprops(labels)  # 这里是取得labels的属性，属性有许多\n","    save_indexs = []\n","    num = labels.max()  # 找白色部分的连通域有几个\n","    print('白色区域数量', num)\n","    del_array = np.array([0] * (num + 1))\n","    for k in range(num):  # 这里是找最大的那个白色连通域的标号\n","        if k == 0:\n","            initial_area = jj[0].area\n","            save_index = 1  # 初始保留第一个连通域\n","            if save_index not in save_indexs:\n","                save_indexs.append(save_index)\n","        else:\n","            k_area = jj[k].area  # 将元组转换成array\n","            if initial_area < k_area:\n","                initial_area = k_area\n","                save_index = k + 1  # python从0开始，而连通域标记是从1开始\n","                if save_index not in save_indexs:\n","                    save_indexs.append(save_index)\n","    print('save_index: ', save_indexs)\n","    del_array[save_indexs[-2]] = 1\n","    del_array[save_indexs[-1]] = 1\n","    del_mask = del_array[labels]\n","    return del_mask\n","\n","\n","def measureimg(o_img,t_num=1):\n","    p_img=np.zeros_like(o_img)\n","    # temp_img=morphology.binary_dilation(o_img.astype(\"bool\"),iterations=2)\n","    testa1 = measure.label(o_img.astype(\"bool\"))\n","    props = measure.regionprops(testa1)\n","    numPix = []\n","    for ia in range(len(props)):\n","        numPix += [props[ia].area]\n","    # print(numPix)\n","    # 像素最多的连通区域及其指引\n","    for i in range(0,t_num):\n","        index = numPix.index(max(numPix)) + 1\n","        p_img[testa1 == index]=o_img[testa1 == index]\n","        numPix[index-1]=0\n","    return p_img"],"metadata":{"id":"OOChlP16bcxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#mutiscaleUnet"],"metadata":{"id":"b74ok_f7anNS"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ChannelAttention2(nn.Module):\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention2, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","\n","        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n","        self.relu1 = nn.ReLU()\n","        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n","        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)*x\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=3):\n","        super(SpatialAttention, self).__init__()\n","\n","        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n","        padding = 3 if kernel_size == 7 else 1\n","\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        y = torch.cat([avg_out, max_out], dim=1)\n","        y = self.conv1(y)\n","        return self.sigmoid(y) * x\n","\n","class CBAM(nn.Module):\n","    def __init__(self,in_channels):\n","        super(CBAM, self).__init__()\n","\n","        self.ca = ChannelAttention2(in_channels, 16)\n","        self.sa = SpatialAttention(7)\n","\n","    def forward(self, x):\n","        cx = self.ca(x)\n","        out = self.sa(cx)\n","\n","        return out\n","\n","\n","\n","class conv_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(conv_block, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class conv_block2(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(conv_block2, self).__init__()\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","\n","        self.conv1x1 = nn.Conv2d(out_channels * 3, out_channels, kernel_size=1)\n","        self.conv1x2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        x1 = self.conv1(x)\n","        # print(\"here:\",x1.shape)\n","        x2 = self.conv2(x1)\n","        # print(\"here1:\", x1.shape)\n","        x3 = self.conv3(x2)\n","        # print(\"here2:\", x1.shape)\n","        x_out = torch.cat([x1, x2, x3], dim=1)\n","        # print(\"here3:\", x1.shape)\n","        x_res = self.conv1x1(x_out)\n","        # print(\"here4:\", x1.shape)\n","        x_out = self.conv1x2(x)\n","        return x_res+x_out\n","\n","class conv_block4(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(conv_block4, self).__init__()\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","\n","        self.conv1x1 = nn.Conv2d(out_channels * 3, out_channels, kernel_size=1)\n","        self.conv1x2 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.attention = CBAM2(out_channels * 3)\n","\n","    def forward(self, x):\n","        x1 = self.conv1(x)\n","        # print(\"here:\",x1.shape)\n","        x2 = self.conv2(x1)\n","        # print(\"here1:\", x1.shape)\n","        x3 = self.conv3(x2)\n","        # print(\"here2:\", x1.shape)\n","        x_out = torch.cat([x1, x2, x3], dim=1)\n","        # print(\"here3:\", x1.shape)\n","        x_out1 = self.attention(x_out)\n","        x_res = self.conv1x1(x_out1)\n","        # print(\"here4:\", x1.shape)\n","        x_out = self.conv1x2(x)\n","        return x_res+x_out\n","\n","class conv_block3(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(conv_block3, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm3d(out_channels),\n","            nn.ReLU(inplace=True),)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","class MultiScaleFeatureFusionConvBlock3d(nn.Module):\n","    '''\n","    多尺度特征融合模块+卷积模块\n","    '''\n","\n","    def __init__(self, in_channels, out_channels):\n","        super(MultiScaleFeatureFusionConvBlock3d, self).__init__()\n","\n","        self.out_channels_split = out_channels // 4\n","        self.conv1_in = nn.Conv3d(in_channels, out_channels, [1, 1, 1], stride=1, padding=0)\n","        self.conv3_addition_2 = conv_block3(self.out_channels_split, self.out_channels_split)\n","        self.conv3_addition_3 = conv_block3(self.out_channels_split, self.out_channels_split)\n","        self.conv3_addition_4 = conv_block3(self.out_channels_split, self.out_channels_split)\n","        self.conv1_out = nn.Conv3d(out_channels, out_channels, [1, 1, 1], stride=1, padding=0)\n","\n","        self.conv3 = conv_block3(out_channels, out_channels)\n","\n","    def forward(self, x):\n","        f = self.conv1_in(x)\n","        f_1 = f[:, 0: self.out_channels_split, :, :, :]\n","        f_2 = self.conv3_addition_2(f[:, self.out_channels_split: 2 * self.out_channels_split, :, :, :])\n","        f_3 = self.conv3_addition_3(f[:, 2 * self.out_channels_split: 3 * self.out_channels_split, :, :, :] + f_2)\n","        f_4 = self.conv3_addition_4(f[:, 3 * self.out_channels_split: 4 * self.out_channels_split, :, :, :] + f_3)\n","        fusion = f_1 + f_2 + f_3 + f_4\n","        f_1 = f_1 + fusion\n","        f_2 = f_2 + fusion\n","        f_3 = f_3 + fusion\n","        f_4 = f_4 + fusion\n","        return self.conv3(self.conv1_out(torch.cat((f_1, f_2, f_3, f_4), dim=1)))\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=1):\n","        super(UNet, self).__init__()\n","        n = 32\n","        filters = [n, n*2, n*4, n*8, n*16]\n","        self.conv0 = conv_block2(in_channels, filters[0])\n","        self.conv1 = conv_block2(filters[0], filters[1])\n","        self.conv2 = conv_block2(filters[1], filters[2])\n","        self.conv3 = conv_block2(filters[2], filters[3])\n","        # self.conv4 = conv_block(filters[3], filters[4])\n","\n","        # self.conv5 = conv_block(filters[4], filters[3])\n","        self.conv6 = conv_block2(filters[3], filters[2])\n","        self.conv7 = conv_block2(filters[2], filters[1])\n","        self.conv8 = conv_block2(filters[1], filters[0])\n","\n","        self.maxpool1 = nn.MaxPool3d(kernel_size=[2, 2, 2])\n","        self.maxpool2 = nn.MaxPool3d(kernel_size=[2, 2, 2])\n","        self.maxpool3 = nn.MaxPool3d(kernel_size=[2, 2, 2])\n","        # self.maxpool4 = nn.MaxPool3d(2)\n","\n","        # self.transconv4 = nn.ConvTranspose3d(filters[4], filters[4] // 2, kernel_size=2, stride=2)\n","        self.transconv3 = nn.ConvTranspose3d(filters[3], filters[3] // 2, kernel_size=2, stride=2)\n","        self.transconv2 = nn.ConvTranspose3d(filters[2], filters[2] // 2, kernel_size=2, stride=2)\n","        self.transconv1 = nn.ConvTranspose3d(filters[1], filters[1] // 2, kernel_size=2, stride=2)\n","\n","        self.conv1x1 = nn.Conv3d(filters[0], out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        # encoder #\n","        x = self.conv0(x)\n","        print(x.shape)\n","        x_cat1 = x\n","        x = self.maxpool1(x)\n","\n","        x = self.conv1(x)\n","        print(x.shape)\n","        x_cat2 = x\n","        x = self.maxpool2(x)\n","\n","        x = self.conv2(x)\n","        print(x.shape)\n","        x_cat3 = x\n","        x = self.maxpool3(x)\n","\n","        x = self.conv3(x)\n","        print(x.shape)\n","        # x_cat4 = x\n","        # x = self.maxpool4(x)\n","        #\n","        # x = self.conv4(x)\n","\n","        # decoder #\n","        # x_trans4 = self.transconv4(x)\n","        # x = torch.cat([x_cat4, x_trans4], dim=1)\n","        # x = self.conv5(x)\n","\n","        x_trans3 = self.transconv3(x)\n","        print(x_trans3.shape)\n","\n","        x = torch.cat([x_cat3, x_trans3], dim=1)\n","        x = self.conv6(x)\n","        print(x.shape)\n","        x_trans2 = self.transconv2(x)\n","        x = torch.cat([x_cat2, x_trans2], dim=1)\n","        x = self.conv7(x)\n","        print(x.shape)\n","        x_trans1 = self.transconv1(x)\n","        x = torch.cat([x_cat1, x_trans1], dim=1)\n","        x = self.conv8(x)\n","        print(x.shape)\n","        output = self.conv1x1(x)\n","\n","        return output\n","##################################################################\n","class BasicConv(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n","        super(BasicConv, self).__init__()\n","        self.out_channels = out_planes\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n","        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n","        self.relu = nn.ReLU() if relu else None\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.bn is not None:\n","            x = self.bn(x)\n","        if self.relu is not None:\n","            x = self.relu(x)\n","        return x\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","class ChannelGate(nn.Module):\n","    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n","        super(ChannelGate, self).__init__()\n","        self.gate_channels = gate_channels\n","        self.mlp = nn.Sequential(\n","            Flatten(),\n","            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n","            nn.ReLU(),\n","            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n","            )\n","        self.pool_types = pool_types\n","    def forward(self, x):\n","        channel_att_sum = None\n","        for pool_type in self.pool_types:\n","            if pool_type=='avg':\n","                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( avg_pool )\n","            elif pool_type=='max':\n","                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( max_pool )\n","            elif pool_type=='lp':\n","                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( lp_pool )\n","            elif pool_type=='lse':\n","                # LSE pool only\n","                lse_pool = logsumexp_2d(x)\n","                channel_att_raw = self.mlp( lse_pool )\n","\n","            if channel_att_sum is None:\n","                channel_att_sum = channel_att_raw\n","            else:\n","                channel_att_sum = channel_att_sum + channel_att_raw\n","\n","        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n","        return x * scale\n","\n","def logsumexp_2d(tensor):\n","    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n","    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n","    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n","    return outputs\n","\n","class ChannelPool(nn.Module):\n","    def forward(self, x):\n","        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n","\n","class SpatialGate(nn.Module):\n","    def __init__(self):\n","        super(SpatialGate, self).__init__()\n","        kernel_size = 7\n","        self.compress = ChannelPool()\n","        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n","    def forward(self, x):\n","        x_compress = self.compress(x)\n","        x_out = self.spatial(x_compress)\n","        scale = F.sigmoid(x_out) # broadcasting\n","        return x * scale\n","\n","class CBAM2(nn.Module):\n","    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n","        super(CBAM2, self).__init__()\n","        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n","        self.no_spatial=no_spatial\n","        if not no_spatial:\n","            self.SpatialGate = SpatialGate()\n","    def forward(self, x):\n","        x_out = self.ChannelGate(x)\n","        if not self.no_spatial:\n","            x_out = self.SpatialGate(x_out)\n","        return x_out\n"],"metadata":{"id":"gtUVUMjoameg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.autograd import Variable\n","\n","img = torch.randn(1,32,128,128)\n","img = img.cuda()\n","\n","net = UNet(1,2).cuda()\n","\n","#out = net(img)\n","\n","#print(out.size())"],"metadata":{"id":"UUZiV0dkcNt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqpsnF4Nc9ZW","executionInfo":{"status":"ok","timestamp":1651929209665,"user_tz":-480,"elapsed":10,"user":{"displayName":"马荣康","userId":"08078634112129011632"}},"outputId":"0d42e67f-c7eb-494d-bdbf-1c852be7d62c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["UNet(\n","  (conv0): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (conv1): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (conv2): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (conv3): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (conv6): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (conv7): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (conv8): conv_block2(\n","    (conv1): Sequential(\n","      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv2): Sequential(\n","      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv3): Sequential(\n","      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv1x1): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1x2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (maxpool1): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n","  (maxpool2): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n","  (maxpool3): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n","  (transconv3): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","  (transconv2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","  (transconv1): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","  (conv1x1): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",")\n"]}]},{"cell_type":"markdown","source":["#UNet"],"metadata":{"id":"H9_3tnz2aYiq"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","# class conv_block(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(conv_block, self).__init__()\n","#\n","#         self.conv = nn.Sequential(\n","#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","#             nn.BatchNorm2d(out_channels),\n","#             nn.ReLU(inplace=True),\n","#             nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","#             nn.BatchNorm2d(out_channels),\n","#             nn.ReLU(inplace=True))\n","#\n","#     def forward(self, x):\n","#         x = self.conv(x)\n","#         return x\n","\n","\n","class trans_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(trans_block, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=1):\n","        super(UNet, self).__init__()\n","        n = 32\n","        filters = [n, n*2, n*4, n*8, n*16]\n","        self.conv0 = conv_block4(in_channels, filters[0])\n","        self.conv1 = conv_block4(filters[0], filters[1])\n","        self.conv2 = conv_block4(filters[1], filters[2])\n","        self.conv3 = conv_block4(filters[2], filters[3])\n","        self.conv4 = conv_block4(filters[3], filters[4])\n","\n","        self.conv5 = conv_block4(filters[4], filters[3])\n","        self.conv6 = conv_block4(filters[3], filters[2])\n","        self.conv7 = conv_block4(filters[2], filters[1])\n","        self.conv8 = conv_block4(filters[1], filters[0])\n","\n","        self.maxpool1 = nn.MaxPool2d(2)\n","        self.maxpool2 = nn.MaxPool2d(2)\n","        self.maxpool3 = nn.MaxPool2d(2)\n","        self.maxpool4 = nn.MaxPool2d(2)\n","\n","        self.transconv4 = nn.ConvTranspose2d(filters[4], filters[4] // 2, kernel_size=2, stride=2)\n","        self.transconv3 = nn.ConvTranspose2d(filters[3], filters[3] // 2, kernel_size=2, stride=2)\n","        self.transconv2 = nn.ConvTranspose2d(filters[2], filters[2] // 2, kernel_size=2, stride=2)\n","        self.transconv1 = nn.ConvTranspose2d(filters[1], filters[1] // 2, kernel_size=2, stride=2)\n","\n","        self.conv1x1 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n","\n","        # Initialize convolutions' parameters\n","        self.init_conv2d()\n","\n","    def init_conv2d(self):\n","        \"\"\"\n","        Initialize convolution parameters.\n","        \"\"\"\n","        for c in self.children():\n","            if isinstance(c, nn.Conv2d):\n","                nn.init.xavier_uniform_(c.weight)\n","                nn.init.constant_(c.bias, 0.)\n","\n","    def forward(self, x):\n","        # encoder #\n","        x = self.conv0(x)\n","        x_cat1 = x\n","        x = self.maxpool1(x)\n","\n","        x = self.conv1(x)\n","        x_cat2 = x\n","        x = self.maxpool2(x)\n","\n","        x = self.conv2(x)\n","        x_cat3 = x\n","        x = self.maxpool3(x)\n","\n","        x = self.conv3(x)\n","        x_cat4 = x\n","        x = self.maxpool4(x)\n","\n","        x = self.conv4(x)\n","\n","        # decoder #\n","        x_trans4 = self.transconv4(x)\n","        x = torch.cat([x_cat4, x_trans4], dim=1)\n","        x = self.conv5(x)\n","\n","        x_trans3 = self.transconv3(x)\n","        x = torch.cat([x_cat3, x_trans3], dim=1)\n","        x = self.conv6(x)\n","\n","        x_trans2 = self.transconv2(x)\n","        x = torch.cat([x_cat2, x_trans2], dim=1)\n","        x = self.conv7(x)\n","\n","        x_trans1 = self.transconv1(x)\n","        x = torch.cat([x_cat1, x_trans1], dim=1)\n","        x = self.conv8(x)\n","\n","        output = self.conv1x1(x)\n","\n","        return output\n"],"metadata":{"id":"MXuUg0aqae_a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#utils"],"metadata":{"id":"-rW5LjTRfyk2"}},{"cell_type":"code","source":["import torch\n","import logging\n","import torch.nn as nn\n","import numpy as np\n","from skimage import measure\n","from torch._utils import _accumulate\n","from torch import randperm\n","from scipy.ndimage import morphology\n","\n","\n","def random_split(dataset, lengths, inds=None, israndom=True):\n","    r\"\"\"\n","    Randomly split a dataset into non-overlapping new datasets of given lengths.\n","\n","    Arguments:\n","        dataset (Dataset): Dataset to be split\n","        lengths (sequence): lengths of splits to be produced\n","    \"\"\"\n","    if sum(lengths) != len(dataset):\n","        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n","\n","    if israndom:\n","        indices = randperm(sum(lengths)).tolist()\n","        print(indices)\n","    else:\n","        indices = inds\n","\n","    return [torch.utils.data.Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n","\n","\n","def expand_as_one_hot(input, C, ignore_index=None):\n","    \"\"\"\n","    Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\n","    It is assumed that the batch dimension is present.\n","    Args:\n","        input (torch.Tensor): 3D/4D input image\n","        C (int): number of channels/labels\n","        ignore_index (int): ignore index to be kept during the expansion\n","    Returns:\n","        4D/5D output torch.Tensor (NxCxSPATIAL)\n","    \"\"\"\n","    assert input.dim() == 4\n","\n","    # expand the input tensor to Nx1xSPATIAL before scattering\n","    input = input.unsqueeze(1)\n","    # create output tensor shape (NxCxSPATIAL)\n","    shape = list(input.size())\n","    shape[1] = C\n","\n","    if ignore_index is not None:\n","        # create ignore_index mask for the result\n","        mask = input.expand(shape) == ignore_index\n","        # clone the src tensor and zero out ignore_index in the input\n","        input = input.clone()\n","        input[input == ignore_index] = 0\n","        # scatter to get the one-hot tensor\n","        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n","        # bring back the ignore_index in the result\n","        result[mask] = ignore_index\n","        return result\n","    else:\n","        # scatter to get the one-hot tensor\n","        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n","\n","\n","def random_split(dataset, lengths, inds=None, israndom=True):\n","    r\"\"\"\n","    Randomly split a data into non-overlapping new datasets of given lengths.\n","\n","    Arguments:\n","        dataset (Dataset): Dataset to be split\n","        lengths (sequence): lengths of splits to be produced\n","    \"\"\"\n","    if sum(lengths) != len(dataset):\n","        raise ValueError(\"Sum of input lengths does not equal the length of the input data!\")\n","\n","    if israndom:\n","        indices = randperm(sum(lengths)).tolist()\n","        print(indices)\n","    else:\n","        indices = inds\n","\n","    return [torch.utils.data.Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","\n","def logger(filename, verbosity=1, name=None):\n","    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n","    formatter = logging.Formatter(\n","        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n","    )\n","    logger = logging.getLogger(name)\n","    logger.setLevel(level_dict[verbosity])\n","\n","    fh = logging.FileHandler(filename, \"w\")\n","    fh.setFormatter(formatter)\n","    logger.addHandler(fh)\n","\n","    sh = logging.StreamHandler()\n","    sh.setFormatter(formatter)\n","    logger.addHandler(sh)\n","    return logger\n","\n","\n","def iou_score(output, target):\n","    smooth = 1e-5\n","    if torch.is_tensor(output):\n","        output = torch.sigmoid(output).data.cpu().round().numpy()\n","    if torch.is_tensor(target):\n","        target = target.data.cpu().numpy()\n","    output_ = output > 0.5\n","    target_ = target > 0.5\n","    intersection = (output_ & target_).sum()\n","    union = (output_ | target_).sum()\n","\n","    return (intersection + smooth) / (union + smooth)\n","\n","\n","def dice_coeff(output, target):\n","    smooth = 1e-5\n","\n","    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n","    target = target.view(-1).data.cpu().numpy()\n","    intersection = (output * target).sum()\n","\n","    return (2. * intersection + smooth) / \\\n","        (output.sum() + target.sum() + smooth)\n","\n","\n","class Evaluator(object):\n","    def __init__(self, num_class):\n","        self.num_class = num_class\n","        self.confusion_matrix = np.zeros((self.num_class,)*2)\n","\n","    def Precision(self):\n","        precision = np.diag(self.confusion_matrix)[0]/self.confusion_matrix[:, 0].sum()\n","        return precision\n","\n","    def Recall(self):\n","        recall = np.diag(self.confusion_matrix)[0]/self.confusion_matrix[0, :].sum()\n","        return recall\n","\n","    def Specificity(self):\n","        specificity = np.diag(self.confusion_matrix)[1]/(self.confusion_matrix[0, 1]+self.confusion_matrix[1, 0])\n","        return specificity\n","\n","    def F1score(self):\n","        prec = self.Precision()\n","        rec = self.Recall()\n","        f1_score = (2*prec*rec)/(prec+rec)\n","        return f1_score\n","\n","    def F2score(self):\n","        prec = self.Precision()\n","        rec = self.Recall()\n","        f2_score = (5*prec*rec)/(4*prec+rec)\n","        return f2_score\n","\n","    def Intersection_over_Union(self):\n","        iou = np.diag(self.confusion_matrix)[0]/(self.confusion_matrix[0,0]+self.confusion_matrix[1,0]+self.confusion_matrix[0,1])\n","        return iou\n","\n","    def Pixel_Accuracy(self):\n","        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n","        return Acc\n","\n","    def Pixel_Accuracy_Class(self):\n","        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n","        Acc = np.nanmean(Acc)\n","        return Acc\n","\n","    def Mean_Intersection_over_Union(self):\n","        MIoU = np.diag(self.confusion_matrix) / (\n","                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n","                    np.diag(self.confusion_matrix))\n","        MIoU = np.nanmean(MIoU)\n","        return MIoU\n","\n","    def Frequency_Weighted_Intersection_over_Union(self):\n","        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n","        iu = np.diag(self.confusion_matrix) / (\n","                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n","                    np.diag(self.confusion_matrix))\n","\n","        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n","        return FWIoU\n","\n","    def _generate_matrix(self, gt_image, pre_image):\n","        mask = (gt_image >= 0) & (gt_image < self.num_class)\n","        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n","        count = np.bincount(label, minlength=self.num_class**2)\n","        confusion_matrix = count.reshape(self.num_class, self.num_class)\n","        print(confusion_matrix)\n","        return confusion_matrix\n","\n","    def add_batch(self, gt_image, pre_image):\n","        assert gt_image.shape == pre_image.shape\n","        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n","\n","    def reset(self):\n","        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n","\n","\n","def adjust_lr(optimizer, init_lr, epoch, decay_rate=0.1, decay_epoch=30):\n","    decay = decay_rate ** (epoch // decay_epoch)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] *= decay\n","\n","\n","def clip_gradient(optimizer, grad_clip):\n","    \"\"\"\n","    For calibrating misalignment gradient via cliping gradient technique\n","    :param optimizer:\n","    :param grad_clip:\n","    :return:\n","    \"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group['params']:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)\n","\n","\n","def universal3Dlargestregion(deal):\n","    \"\"\"找到3D丈量最大连通域,输出为值为1的mask.deal:输入的3D张量\"\"\"\n","    labels = measure.label(deal, connectivity=3)  # 找白色区域的8连通域，并给予每个连通域标号，connectivity为ndarry的维数，三维数组故为3\n","    jj = measure.regionprops(labels)  # 这里是取得labels的属性，属性有许多\n","    save_indexs = []\n","    num = labels.max()  # 找白色部分的连通域有几个\n","    print('白色区域数量', num)\n","    del_array = np.array([0] * (num + 1))\n","    for k in range(num):  # 这里是找最大的那个白色连通域的标号\n","        if k == 0:\n","            initial_area = jj[0].area\n","            save_index = 1  # 初始保留第一个连通域\n","            if save_index not in save_indexs:\n","                save_indexs.append(save_index)\n","        else:\n","            k_area = jj[k].area  # 将元组转换成array\n","            if initial_area < k_area:\n","                initial_area = k_area\n","                save_index = k + 1  # python从0开始，而连通域标记是从1开始\n","                if save_index not in save_indexs:\n","                    save_indexs.append(save_index)\n","    print('save_index: ', save_indexs)\n","    del_array[save_indexs[-2]] = 1\n","    del_array[save_indexs[-1]] = 1\n","    del_mask = del_array[labels]\n","    return del_mask\n","\n","\n","def measureimg(o_img,t_num=1):\n","    p_img=np.zeros_like(o_img)\n","    # temp_img=morphology.binary_dilation(o_img.astype(\"bool\"),iterations=2)\n","    testa1 = measure.label(o_img.astype(\"bool\"))\n","    props = measure.regionprops(testa1)\n","    numPix = []\n","    for ia in range(len(props)):\n","        numPix += [props[ia].area]\n","    # print(numPix)\n","    # 像素最多的连通区域及其指引\n","    for i in range(0,t_num):\n","        index = numPix.index(max(numPix)) + 1\n","        p_img[testa1 == index]=o_img[testa1 == index]\n","        numPix[index-1]=0\n","    return p_img"],"metadata":{"id":"ZhRHRZ5af3EO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#loss"],"metadata":{"id":"uzaMXnUWk0HG"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn.functional import one_hot\n","\n","\n","class WCEDCELoss(nn.Module):\n","    def __init__(self, num_classes=4, inter_weights=0.5, intra_weights=None, device='cuda'):\n","        super(WCEDCELoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss(weight=intra_weights)\n","        self.num_classes = num_classes\n","        self.intra_weights = intra_weights\n","        self.inter_weights = inter_weights\n","        self.device = device\n","\n","    def dice_loss(self, prediction, target, weights):\n","        \"\"\"Calculating the dice loss\n","        Args:\n","            prediction = predicted image\n","            target = Targeted image\n","        Output:\n","            dice_loss\"\"\"\n","        smooth = 1e-5\n","\n","        prediction = torch.softmax(prediction, dim=1)\n","        batchsize = target.size(0)\n","        num_classes = target.size(1)\n","        prediction = prediction.view(batchsize, num_classes, -1)\n","        target = target.view(batchsize, num_classes, -1)\n","\n","        intersection = (prediction * target)\n","\n","        dice = (2. * intersection.sum(2) + smooth) / (prediction.sum(2) + target.sum(2) + smooth)\n","        # print('dice: ', dice)\n","        dice_loss = 1 - dice.sum(0) / batchsize\n","        weighted_dice_loss = dice_loss * weights\n","\n","        # print(dice_loss, weighted_dice_loss)\n","        return weighted_dice_loss.mean()\n","\n","    def forward(self, pred, label):\n","        \"\"\"Calculating the loss and metrics\n","            Args:\n","                prediction = predicted image\n","                target = Targeted image\n","                metrics = Metrics printed\n","                bce_weight = 0.5 (default)\n","            Output:\n","                loss : dice loss of the epoch \"\"\"\n","        cel = self.ce_loss(pred, label)\n","        label_onehot = one_hot(label, num_classes=self.num_classes).permute(0, 3, 1, 2).contiguous()\n","\n","        if self.intra_weights == None:\n","            intra_weights = torch.zeros([self.num_classes]).to(self.device)\n","            for item in range(self.num_classes):\n","                intra_weights[item] = len(label.view(-1)) / (len(label[label == item].view(-1)) + 1e-5)\n","        else:\n","            intra_weights = self.intra_weights\n","        # print('weights: ', intra_weights)\n","        dicel = self.dice_loss(pred, label_onehot, intra_weights)\n","        # print('ce: ', cel, 'dicel: ', dicel)\n","        loss = cel * self.inter_weights + dicel * (1 - self.inter_weights)\n","\n","        return loss"],"metadata":{"id":"442qTDFKky96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wcedceloss = WCEDCELoss()\n","label = torch.randint(low=0, high=4, size=[2, 224, 224]).cuda()\n","print(one_hot(label, 4).shape)\n","prediction = torch.randn([2, 4, 224, 224]).cuda()\n","loss = wcedceloss(pred=prediction, label=label)\n","print('loss: ', loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMyblfSpk9kd","executionInfo":{"status":"ok","timestamp":1651929210375,"user_tz":-480,"elapsed":4,"user":{"displayName":"马荣康","userId":"08078634112129011632"}},"outputId":"e697af2e-cf65-4f34-ca7a-1d88a00ce45a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 224, 224, 4])\n","loss:  tensor(2.3606, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["#main"],"metadata":{"id":"9T2nVLonZpv7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNHmeQoOYSJr"},"outputs":[],"source":["from torch.utils.data.dataset import Dataset\n","import os\n","from torchvision.transforms import *\n","from PIL import Image\n","import random\n","import torch\n","import numpy as np\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","source":["def transform():\n","    return Compose([\n","        Resize([512, 512], Image.NEAREST),\n","        ToTensor()\n","    ])\n","\n","\n","def label_transform():\n","    return Compose([\n","        Resize([512, 512], Image.NEAREST),\n","    ])\n","\n","def augmentation_transform():\n","    return Compose([\n","        RandomHorizontalFlip(),\n","        RandomVerticalFlip(),\n","        RandomRotation(degrees=180),\n","        ColorJitter(),\n","        RandomResizedCrop(size=512)\n","    ])\n","\n","class Test1Dataset(Dataset):\n","    def __init__(self, image_dir = '', stage = 'train', augmentation = False):\n","        super(Test1Dataset, self).__init__()\n","        self.stage = stage\n","        self.augmentation = augmentation\n","\n","        image_path = image_dir +'/2d_images'\n","        label_path = image_dir +'/2d_masks'\n","\n","        self.flair_image_paths = [os.path.join(image_path, x)\n","                                  for x in os.listdir(os.path.join(image_path))\n","\n","                                  if x.endswith('.tif')]\n","\n","\n","\n","        self.label_paths = [os.path.join(label_path, x)\n","                            for x in os.listdir(label_path)\n","\n","                            if x.endswith('tif')]\n","\n","\n","        # print(self.flair_image_paths)\n","        # print(self.label_paths)\n","\n","        self.transform = transform()\n","        self.label_transform = label_transform()\n","        self.augmentation_transform = augmentation_transform()\n","\n","    def __getitem__(self, index):\n","        flair_image = Image.open(self.flair_image_paths[index])\n","\n","        label = Image.open(self.label_paths[index])\n","\n","        if self.augmentation and self.stage == 'train':\n","            factor = random.choice([0,1])\n","            if factor:\n","                flair_image, label = self.augmentation_transform(flair_image,label)\n","\n","        flair_image = self.transform(flair_image)\n","\n","        label = self.label_transform(label)\n","        # image = torch.cat([flair_image, t1_image, t1ce_image, t2_image])\n","        image = flair_image\n","        label = torch.from_numpy(np.array(label)).float().unsqueeze(0)\n","\n","        label[label > 1] = 1\n","\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.label_paths)"],"metadata":{"id":"cVjK4v4FYtl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = '/content/drive/MyDrive/dataset'\n","\n","dataset = Test1Dataset(image_dir = dataset_path,augmentation = False)\n","print(dataset)\n","dataloader = DataLoader(dataset, batch_size = 2, shuffle = True, num_workers = 0)\n","\n","print(len(dataset))\n","#for image, label in dataloader:\n","#  print(image.shape, label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bk9OehYIYvDz","executionInfo":{"status":"ok","timestamp":1651929212728,"user_tz":-480,"elapsed":2356,"user":{"displayName":"马荣康","userId":"08078634112129011632"}},"outputId":"6c3694a0-7670-48b2-d6fc-a5c603e757fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.Test1Dataset object at 0x7f37e0e4ca10>\n","267\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import time\n","import os\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import logging\n","import torch.nn as nn\n","import numpy as np\n","from skimage import measure\n","from torch._utils import _accumulate\n","from torch import randperm\n","from scipy.ndimage import morphology\n","\n","def logger(filename, verbosity=1, name=None):\n","    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n","    formatter = logging.Formatter(\n","        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n","    )\n","    logger = logging.getLogger(name)\n","    logger.setLevel(level_dict[verbosity])\n","\n","    fh = logging.FileHandler(filename, \"w\")\n","    fh.setFormatter(formatter)\n","    logger.addHandler(fh)\n","\n","    sh = logging.StreamHandler()\n","    sh.setFormatter(formatter)\n","    logger.addHandler(sh)\n","    return logger\n"],"metadata":{"id":"8kfaegskZ8Up"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Num_epochs = 10\n","Num_batchsize = 3\n","Learningrate = 0.0003\n","device = torch.device(\"cuda\" )\n","print(device)\n","Imagedir =  '/content/drive/MyDrive/dataset/'\n","\n","Architecture = ['Unet']\n","architecture = Architecture[0]\n","\n","AUGMENTATION = False\n","WCEDCELOSS = False\n","Deepsupervision = False\n","\n","############################load data#########################\n","set1 = Test1Dataset(image_dir=Imagedir , stage='train', augmentation=AUGMENTATION)\n","\n","\n","train_set = set1\n","\n","train_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=Num_batchsize, shuffle=True, pin_memory=True)\n","\n","############################load the net#####################\n","\n","\n","inet = UNet(in_channels=1, out_channels=2)\n","print(\"#parameters:\", sum(param.numel() for param in inet.parameters()))\n","\n","inet = inet.to(device)\n","##################loss function and optimization##############\n","\n","if WCEDCELOSS:\n","\n","    criterion = WCEDCELoss(intra_weights=torch.tensor([1., 5.]).to(device), device=device, inter_weights=0.5)\n","else:\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","optimizer = optim.AdamW(inet.parameters(), lr=Learningrate, betas=(0.9, 0.999))\n","\n","#######Train the net###################3\n","\n","results = {'loss': [], 'dice': [], 'iou': [], 'val_loss': [], 'val_dice': [], 'val_iou': []}\n","\n","\n","\n","for epoch in range(1, Num_epochs + 1):\n","    epochresults = {'loss': [], 'dice': [], 'iou': [], 'val_loss': [], 'val_dice': [], 'val_iou': []}\n","    inet.train()\n","    for iteration, data in enumerate(train_loader):\n","        image, label = data\n","        #print(\"*********************\",image.shape,label.shape)\n","        image = image.to(device)\n","        label = label.to(device)\n","        optimizer.zero_grad()\n","\n","        if Deepsupervision:\n","            pred, pred1, pred2, pred3, pred4 = inet(image)\n","            loss0 = criterion(pred, label.squeeze(1).long())\n","            loss1 = criterion(pred1, F.interpolate(label, scale_factor=1. / 2., mode='bilinear').squeeze(1).long())\n","            loss2 = criterion(pred2, F.interpolate(label, scale_factor=1. / 4., mode='bilinear').squeeze(1).long())\n","            loss3 = criterion(pred3, F.interpolate(label, scale_factor=1. / 8., mode='bilinear').squeeze(1).long())\n","            loss4 = criterion(pred4, F.interpolate(label, scale_factor=1. / 16., mode='bilinear').squeeze(1).long())\n","            loss = 0.4 * loss0 + 0.3 * loss1 + 0.2 * loss2 + 0.05 * loss3 + 0.05 * loss4\n","        else:\n","            pred = inet(image.float())\n","            #print(\"#######\",pred.shape)\n","            loss = criterion(pred, label.squeeze(1).long())\n","\n","        loss.backward()\n","        optimizer.step()\n","        ########loss of each iteration########\n","        if iteration % 100 == 0:\n","            print(\"Train: Epoch/Epoches {}/{}\\t\"\n","                        \"iteration/iterations {}/{}\\t\"\n","                        \"loss {:.3f}\".format(epoch, Num_epochs, iteration, len(train_loader), loss.item()))\n","\n","        epochresults['loss'].append(loss.item())\n","\n","    results['loss'].append(np.mean(epochresults['loss']))# 每个epoch的迭代loss的平均结果保存\n","\n","    ######Average loss of each epoch########\n","    print(\"Average: Epoch/Epoches {}/{}\\t\"\n","                \"train epoch loss {:.3f}\\n\".format(epoch, Num_epochs, np.mean(epochresults['loss'])))\n","    #######save lastest epoch modle#######\n","    net_model_path = '/content/drive/MyDrive/Models'\n","    \n","    torch.save(inet.state_dict(), net_model_path + \"/net_best_epoch_%d.pth\" % epoch)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKHqkgELjkwg","executionInfo":{"status":"ok","timestamp":1651930719575,"user_tz":-480,"elapsed":1004575,"user":{"displayName":"马荣康","userId":"08078634112129011632"}},"outputId":"a624755e-d66d-4436-d036-3454fdb951d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","#parameters: 13852026\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"]},{"output_type":"stream","name":"stdout","text":["Train: Epoch/Epoches 1/10\titeration/iterations 0/89\tloss 43.097\n","Average: Epoch/Epoches 1/10\ttrain epoch loss 4.985\n","\n","Train: Epoch/Epoches 2/10\titeration/iterations 0/89\tloss 1.424\n","Average: Epoch/Epoches 2/10\ttrain epoch loss 0.739\n","\n","Train: Epoch/Epoches 3/10\titeration/iterations 0/89\tloss 0.886\n","Average: Epoch/Epoches 3/10\ttrain epoch loss 0.445\n","\n","Train: Epoch/Epoches 4/10\titeration/iterations 0/89\tloss 0.488\n","Average: Epoch/Epoches 4/10\ttrain epoch loss 0.353\n","\n","Train: Epoch/Epoches 5/10\titeration/iterations 0/89\tloss 0.319\n","Average: Epoch/Epoches 5/10\ttrain epoch loss 0.437\n","\n","Train: Epoch/Epoches 6/10\titeration/iterations 0/89\tloss 0.244\n","Average: Epoch/Epoches 6/10\ttrain epoch loss 0.365\n","\n","Train: Epoch/Epoches 7/10\titeration/iterations 0/89\tloss 0.349\n","Average: Epoch/Epoches 7/10\ttrain epoch loss 0.378\n","\n","Train: Epoch/Epoches 8/10\titeration/iterations 0/89\tloss 0.267\n","Average: Epoch/Epoches 8/10\ttrain epoch loss 0.324\n","\n","Train: Epoch/Epoches 9/10\titeration/iterations 0/89\tloss 0.232\n","Average: Epoch/Epoches 9/10\ttrain epoch loss 0.312\n","\n","Train: Epoch/Epoches 10/10\titeration/iterations 0/89\tloss 0.257\n","Average: Epoch/Epoches 10/10\ttrain epoch loss 0.889\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ykmOLvbOroff"},"execution_count":null,"outputs":[]}]}